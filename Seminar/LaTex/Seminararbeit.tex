\documentclass[11pt, a4paper]{article}
\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage{float}
\title{Seminararbeit zum Thema Splaybaum} 
\author{Andreas Windorfer, q8633657} 
\date{\today} 





\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	
	\section{Einleitung} 
	Der Splaybaum ist eine von Daniel D. Sleator und Robert E. Tarjan vorgestellte Datenstruktur, die sich bei vielen Praxisanwendungen sehr effizient einsetzen lässt. In dieser Seminararbeit wird zunächst der Aufbau des Splaybaumes thematisiert. Dazu wird kurz der binäre Suchbaum vorgestellt, da dieser die Basis des Splaybaumes liefert. Im Anschluss wird der Operationssatz des Splaybaumes vorgestellt. Außerdem wird noch auf das Laufzeitverhalten beim Splaybaum eingegangen, vor allem im  Bezug auf Operationsfolgen.
	
	\section{Aufbau des Splaybaum}
	Der Splaybaum ist ein binärer Suchbaum mit einem speziellem Operationssatz.
	\subsection{Binärer Suchbaum}
	
	Bei einem binären Suchbaum kann jeder Knoten sowohl einen linken, als auch einen rechten Kindknoten besitzen, weitere sind nicht möglich. Mit Ausnahme der Wurzel besitzt jeder Knoten genau einen Vaterknoten. Natürlich muss jeder Knoten eine Information enthalten, anhand derer er identifiziert werden kann. Diese Information wird als \grqq{}Schlüssel\grqq{} bezeichnet. Für jeden Knoten des Baumes gilt, dass alle sich in seinem linken Teilbaum befindlichen Schlüssel kleiner als der eigene sind, alle Schlüssel des rechten Teilbaumes sind größer. Damit diese Anforderung eingehalten werden kann, muss eine totale Ordnung auf der Menge der Schlüssel gegeben sein.  Abbildung \ref{fig:ioNioSuchbaum} zeigt einen Suchbaum, der diese Anforderung einhält und einen der sie verletzt.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{"bilder/suchbaumIO_NIO"}
		\caption{Links ist ein gültiger binärer Suchbaum, rechts nicht.}
		\label{fig:ioNioSuchbaum}
	\end{figure}
	
	
	\subsubsection {Suchen beim binären Suchbaum}
	\label{sec:Suchen beim binären Suchbaum}
	Beim Suchen eines Schlüssels im binären Suchbaum, läuft man den Pfad von der Wurzel bis zum jeweiligen Knoten ab. Ist der gesuchte Schlüssel kleiner als der des aktuell betrachteten Knotens, wählt man als nächstes den linken Nachfolger aus. Ist der gesuchte Schlüssel größer, wählt man den rechten Nachfolger. Dieses Vorgehen iteriert man solange bis man beim gesuchten Element angelangt ist. Stellt man fest, dass ein benötigter Nachfolger gar nicht vorhanden ist, kann es keinen Knoten mit dem gesuchten Schlüssel im Baum geben.
	Abbildung \ref{fig:binSuchSuchpfad} stellt einen solchen Suchvorgang dar.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{"bilder/binSuchSuchpfad"}
		\caption{Beispielhafter Suchpfad. Hier für den Schlüssel 6.}
		\label{fig:binSuchSuchpfad}
	\end{figure}

	\subsubsection {Einfügen in den binären Suchbaum}
    Zunächst muss der richtige Einfügestelle gefunden werden. Dazu wird eine Suche nach dem einzufügenden Schlüssel 
    durchgeführt. Falls der Schlüssel noch nicht im Baum vorhanden ist, wird man irgendwann an ein Ende der Datenstruktur gelangen, hier ist dann die passende Einfügestelle. 
    
    \begin{figure}[h]
    	\centering
    	\includegraphics[width=0.75\textwidth]{"bilder/SuchbaumEinfügen"}
    	\caption{Einfügen eines Elementes mit Schlüssel 8.}
    	\label{fig:SuchbaumEinfügen}
    \end{figure}
     
	\noindent  Die Stelle an der ein Element eingefügt wird, hängt also nicht nur vom eigenen Schlüssel, sondern vor allem auch von den bereits im Baum vorhandenen Schlüsseln ab. Dies führt dazu, dass Bäume mir der gleichen Schlüsselmenge, in Abhängigkeit von der Einfügereihenfolge, völlig unterschiedliche Strukturen und auch Höhen haben können.
	Abbildung \ref{fig:SuchbaumEingabefolge} stellt dies dar. Es ist sofort einleuchtend, dass
	die Struktur eines Suchbaumes, wesentlichen Einfluss auf die Laufzeit seiner Operationen
	hat.
	
	 \begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{"bilder/SuchbaumEingabefolge"}
		\caption{Einfügereihenfolge links: 3,2,1,5,4. Rechts: 1,2,3,4,5}
		\label{fig:SuchbaumEingabefolge}
	\end{figure}


	\subsection{Besonderheit beim Aufbau des Splaybaum}
	Jede gültige Darstellung eines Splaybaum ist auch eine gültige Darstellung eines binären Suchbaumes, und umgekehrt. Jedoch gehört der Splaybaum zu den selbstorganisierenden Datenstrukturen. Dies bedeutet beim Splaybaum, dass häufig verwendete Elemente weit oben stehen und somit schnell gefunden werden können. Wie das erreicht wird, wird klar wenn man sich mit seinem Operationssatz beschäftigt, was im folgenden Abschnitt passiert. 
	
	\section{Operationen des Splaybaum}
	Natürlich kann man bei einem Splaybaum Elemente suchen, einfügen und löschen.
	Zusätzlich zu diesen Operationen ist es möglich zwei Splaybäume zu vereinigen,
	oder einen Splaybaum in zwei Bäume aufzuteilen. Alle genannten Operationen 
	verwenden intern eine Hilfsoperation splay, die nun als erstes vorgestellt 
	wird.
	
	\subsection{Splay}
	Ziel der splay( key i, tree s) Operation ist es nicht nur einen ausgewählten Knoten zur neuen Wurzel des Baumes zu machen. Zusätzlich wird die Tiefe der Knoten auf dem Zugriffspfad in etwa halbiert. Siehe dazu auch die Abbildungen \ref{fig:halberPfad1} und \ref{fig:halberPfad2} aus \cite{sl2} von Daniel Dominic Sleator and Robert Endre Tarjan. Dazu wird der Operation splay ein Wert i übergeben. Zunächst wird nach einem Knoten x gesucht, dessen Schlüssel i entspricht. Dieser Suchvorgang läuft ab, wie bei Abschnitt \ref{sec:Suchen beim binären Suchbaum} erklärt. Ist die Suche erfolgreich, bezeichnet x den Knoten mit Schlüssel i. Ist ein solcher Knoten nicht im Baum enthalten, bezeichnet x den letzten Knoten, auf dem beim Suchen zugegriffen wurde, so dass dann entweder der Knoten mit dem nächst kleineren oder dem nächsten größerem Schlüssel verwendet wird. Abbildung \ref{fig:splayKnotenSuche} zeigt dies. Nun wird x Schritt für Schritt durch Rotationen nach oben befördert. Dabei werden sechs Fälle unterschieden, wobei jeweils zwei symmetrisch sind. Im folgenden werden nun zunächst die möglichen Rotationen vorgestellt, bevor dann die Arbeitsweise von splay als Ganzes gezeigt wird. Damit zum Ende von splay wieder eine valide Suchbaumstruktur vorhanden ist, ist es wichtig, dass die ursprüngliche Rechts-Links-Struktur erhalten bleibt. Für alle Elemente des Suchbaumes muss also paarweise gelten, das Element, dass sich vor splay weiter rechts befindet, muss dies nach splay immer noch tun. An den folgenden beigefügten Abbildungen, wird man leicht erkennen, dass die Rechts-Links-Beziehung bei jedem Einzelschritt erhalten bleibt.
	 
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.9\textwidth]{"bilder/halberPfad1"}
		\caption{Beispiel 1. Die Tiefen der Knoten halbieren sich in etwa. \cite{sl2}}
		\label{fig:halberPfad1}
	\end{figure}
	\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{"bilder/halberPfad2"}
	\caption{Beispiel 2. Die Tiefen der Knoten halbieren sich in etwa. \cite{sl2}.}
	\label{fig:halberPfad2}
\end{figure}
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{"bilder/splayKnotenSuche"}
		\caption{Beispielhafte Belegung von x, mit i = 1}
		\label{fig:splayKnotenSuche}
	\end{figure}

	\subsubsection{Zig bzw. zag Rotation }
		Eine zig Rotation wird durchgeführt, wenn x direkter linker Nachfolger der Wurzel y ist.
		Dabei wird x zur Wurzel. y wird zum rechten Nachfolger von x. Sollte x bereits einen rechten Nachfolger gehabt haben, wird dieser samt seiner eventuell vorhanden Teilbäume links an y angehängt. Dies ist immer möglich, da an dieser Stelle zuvor x angefügt war.  Man spricht bei diesem Vorgang auch davon, dass rechts über y rotiert wird.
						
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.75\textwidth]{"bilder/zigRotation"}
			\caption{Zig Rotation}
			\label{fig:zigRotation}
		\end{figure}
		
		\noindent Eine zag Rotation ist der symmetrische Fall, wenn x direkter rechter Nachfolger der Wurzel ist. Er ist in Abbildung \ref{fig:zagRotation} dargestellt.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.75\textwidth]{"bilder/zagRotation"}
			\caption{Zag Rotation}
			\label{fig:zagRotation}
		\end{figure}
		  
		
	\subsubsection{Zig-zag bzw. zag-zig Rotation }
	Die Fälle in den x direkt unter der Wurzel liegt, werden also mit zig bzw. zag behandelt. 
	Liegt x nicht direkt unter der Wurzel, so muss auch der Vater von x einen Vater haben. Dieser wird 
	hier als z bezeichnet. Ist x ein linker Nachfolger und y ein rechter Nachfolger wird zig-zag angewendet.
	Hierbei wird zunächst eine zig-Rotation angewendet, also rechts über y rotiert. Nun befindet sich x an
	der Stelle, die zuvor y besetzte. x ist also rechter Nachfolger von z, deshalb wird nun mit zag links über z rotiert. 
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{"bilder/zigZagRotation"}
		\caption{Zig-zag Rotation}
		\label{fig:zigZagRotation}
	\end{figure}

	\noindent Eine zag-zig Rotation ist der symmetrische Fall, wenn x rechter Nachfolger eines Knoten ist, der selbst linker Nachfolger  ist. Er ist in Abbildung \ref{fig:zagZigRotation} dargestellt.
		\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{"bilder/zagZigRotation"}
		\caption{Zag-zig Rotation}
		\label{fig:zagZigRotation}
	\end{figure}

	\subsubsection{Zig-zig bzw. zag-zag Rotation }
	
	Nun bleiben noch die Fälle über, bei denen sowohl y, als auch x linke bzw. rechte Nachfolger
	sind. Hier wird dann mit Zig-zig bzw. Zag-zag gearbeitet. Wichtig ist, dass als Erstes die oberen Knoten rotiert werden. Die Abbildungen   \ref{fig:zigZigRotation}  und \ref{fig:zagZagRotation} demonstrieren dies.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{"bilder/zigZigRotation"}
		\caption{Zig-zig Rotation}
		\label{fig:zigZigRotation}
	\end{figure}
	
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{"bilder/zagZagRotation"}
		\caption{Zag-zag Rotation}
		\label{fig:zagZagRotation}
	\end{figure}
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.75\textwidth]{"bilder/splayGesamt"}
		\caption{Splay Operation. Hier wird splay(2, t) durchgeführt. }
		\label{fig:splayGesamt}
	\end{figure}
	\subsubsection{Beispielhafte Algorithmen für splay}
	In Abbildung  \ref{fig:splayJava} wird eine Java Implementierung angegeben, die sich genau an die obigen Beschreibungen hält.
	\noindent Die Algorithmen in Abbildung \ref{fig:splayAlgorithmusBottomUp} und \ref{fig:splayAlgorithmusTopDown}  entstammen der Arbeit \cite{sl1} von Daniel Dominic Sleator und  Robert Endre Tarjan, in der sie den von ihnen entwickelten Splaybaum vorstellten. Ersterer Algorithmus arbeitet ebenfalls bottom-up. Das heißt, er ordnet den Baum von unten nach oben neu an. Der zweite arbeitet top-down, also von oben nach unten. Die beiden bottom-up Varianten benötigen eine Möglichkeit, auf den Vater eines Knoten zugreifen zu können. Dies kann über einen zusätzlichen Zeiger pro Knoten umgesetzt werden.
	
	\begin{figure}[H]
	\centering
	\includegraphics[width=1.2\textwidth]{"bilder/splayJava"}
	\caption{Eine Java Implementierung von splay. }
	\label{fig:splayJava}
	\end{figure}
	\noindent Um den Algorithmus aus Abbildung  \ref{fig:splayAlgorithmusBottomUp} nachvollziehen zu können, ist es wichtig zu wissen, dass dieser den Zeiger auf x nicht in jedem Schritt mitschreibt. Ein solcher ist zum Ende der Operation sowieso nicht vorhanden. Die Zeiger von x auf seine Teilbäume werden nur einmal zum Schluss gesetzt. Zum Markieren der Teilbäume während der Operation, werden die Zeiger r und l verwendet. Außerdem wird bei ZigZig bzw. ZagZag auf den Zwischenschritt verzichtet.
	
  	 
	\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{"bilder/splayAlgorithmusTopDown"}
	\caption{Eine mögliche top-down Pseudoimplementierung von splay.\cite{sl1} }
	\label{fig:splayAlgorithmusTopDown}
	\end{figure}
	
	\noindent Das in Abbildung \ref{fig:splayAlgorithmusBottomUp} dargestellte Verfahren, merkt sich mit Hilfe eines zusätzlichen Knoten, die beiden späteren Nachfolger des gesuchten Knotens. Die Zeiger r und l merken sich die Stellen an denen zuletzt nach recht bzw. links verzweigt wurde. Diese Stellen werden benötigt, um eventuell vorhandene Teilbäume des gesuchten Knotens, korrekt versetzen zu können.
	Existiert kein passender Knoten, wird wenn möglich, der nächst Kleinere verwendet, ansonsten der nächst Größere.  
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\textwidth]{"bilder/splayAlgorithmusBottomUp"}
		\caption{Eine mögliche bottom-up Pseudoimplementierung von splay. \cite{sl1} }
		\label{fig:splayAlgorithmusBottomUp}
	\end{figure}
	

	\subsection{Suchen}
	Die Operation suchen(key i, tree t) ist nun sehr einfach umzusetzen. Es wird zunächst splay(i, t) aufgerufen und dann der Wert der neuen Wurzel mit i verglichen. Ist ein Knoten mit Schlüssel i im Baum vorhanden, so muss er nach splay(i, t) die Wurzel sein und eine Referenz auf die Wurzel wird zurückgegeben. Ist der Schlüssel nicht im Baum vorhanden wird null zurückgegeben.

	\subsection{Aufteilen}
	Bei aufteilen(key i, tree t ) wird zunächst suchen (i, t) ausgeführt. Im Anschluss wird ein Teilbaum von der Wurzel abgetrennt. Ist der Schlüssel der Wurzel größer als i wird der linke Teilbaum abgetrennt, ansonsten der Rechte. Es werden dann Referenzen auf die beiden resultierenden Bäume zurückgegeben.

	\subsection{Vereinigen} 
	Damit bei vereinigen (tree t1, tree t2) wieder ein Splaybaum entsteht muss gelten, dass der kleinste gespeicherte Schlüssel in t2 größer ist als der größte in t1. Bei der Operation wird zunächst suchen(KeyMax, t1) durchgeführt, wobei KeyMax für den größten möglichen Schlüssel steht. Nach der suchen Operation, ist der Knoten mit dem größten Schlüssel an der Wurzel. Dieser Knoten kann natürlich auch keinen rechten Teilbaum haben, so dass man den Splaybaum t2 nun einfach rechts anfügen kann. Zusätzlich gibt es eine Operation vereinigen(tree t1, key i, tree t2) mit einem dritten Parameter vom Typ key. Für diesen Parameter muss gelten, dass er größer bzw. kleiner als jeder in t1 bzw. t2 vorkommende Schlüssel ist. Dann wird ein Knoten mit Schlüssel i erzeugt und t1 und t2 werden einfach angehängt.

	\subsection{Einfügen}
	Bei einfügen (key i, tree t) wird zunächst aufteilen(i, t) ausgeführt. Im Anschluss wird ein neuer Knoten mit Schlüssel i erzeugt, an den die beiden entstandenen Bäume angehängt werden.


	\subsection{Löschen}
	Bei löschen (key i, tree t) wird zunächst suchen(i, t) ausgeführt. Wurde ein Knoten mit Schlüssel i gefunden, kann dieser entfernt werden. Im Anschluss muss dann noch vereinigen auf den beiden entstandenen Bäumen ausgeführt werden.   

	

	
	% 	\begin{figure}[h]
	% 	\centering
	% 	\includegraphics[width=0.85\textwidth]{"bilder/operationen"}
	% 	\caption{Die weiteren Operationen eines Splaybaumes. }
	% 	\label{fig:operationen}
	% \end{figure}  
 
 \section{Komplexität}
 Anders als andere Suchbäume macht der Splaybaum keine besondere Zusicherung bezüglich seiner maximalen Höhe. Fügt man eine sortierte Folge von n Schlüsseln in den anfangs leeren Baum ein, so erhält man einen Baum mit Höhe n. Abbildung \ref{fig:suchbaumhoeen} stellt dies an der Schlüsselfolge 1 bis 5 dar. Da der Knoten mit dem nächst kleineren Wert zuletzt jeweils als Wurzel eingefügt wurde, endet der Suchvorgang zum Beginn der Einfügeoperation bereits an der Wurzel. Es kommt durch den Aufruf von splay(i, t) innerhalb von einfügen(i, t) also zu keiner Veränderung des Baumes. Wie beim gewöhnlichen binären Suchbaum erhält man bei einer solchen Konstellation eine Liste, an der jedoch oben anstatt unten angefügt wird. 

\noindent Außerdem soll noch die Speicherplatzkomplexität erwähnt werden. Da pro gespeichertem Schlüssel ein Knoten benötigt wird, ist diese  $O(n)$.


 
 
 \begin{figure}[h]
 	\centering
 	\includegraphics[width=0.85\textwidth]{"bilder/suchbaumhoeen"}
 	\caption{Splaybaum in den eine aufwärts sortierte Folge von Schlüsseln eingefügt wurde. }
 	\label{fig:suchbaumhoeen}
 \end{figure}  
 
  
 \subsection {Einzeloperationen}
 Nun benötigt die splay Operation pro Baumebene eine Rotation, damit der gewünschte Knoten die Wurzelposition erreicht. Da die Kosten einer Rotation unabhängig von der Position im Baum sind, ergibt sich als Laufzeitkomplexität für splay $O(n)$. Für jede vorgestellte Operation, mit Ausnahme der Vereinigung mit drei Parametern, gilt, dass nach einem internen Aufruf von splay lediglich noch lokal bei der Wurzel gearbeitet wird. Auch diese Operationen haben also Laufzeit $O(n)$. Bei  vereinigen(tree s1, key i, tree s2) wird lediglich ein Knoten erzeugt und beschrieben. Die Operation hat also Laufzeitkomplexität  $O(1)$.
 
\subsection{Amortisierte Laufzeitanalyse} 
Die Stärke des Splaybaum kommt erst zum Vorschein, wenn man eine amortisierte Laufzeitanalyse durchführt. Zunächst soll diese jedoch erstmals vorgestellt werden.
\subsubsection{Was ist eine amortisierte Laufzeitanalyse ?} 
Bei einer amortisierten Laufzeitanalyse geht es darum  eine möglichst niedrige, obere Schranke für die durchschnittliche Laufzeit einer Operationsfolge im schlechtesten Fall zu ermitteln. Zum Beispiel hat die splay Operation einzeln betrachtet Laufzeit $O(n)$. Nimmt man nun eine Folge von m Operationen an, ergibt sich $O(mn)$. Mit Hilfe einer amortisierten Analyse ist es jedoch gelungen, eine niedrigere obere Schranke zu finden. 

\subsubsection{Bankkontomethode}
Die Bankkontomethode ist eine von drei Methoden, die bei der amortisierten Analyse verwendet werden. Die Idee dabei ist es, günstige Einzeloperationen die teureren subventionieren zu lassen. Dazu zahlen die günstigen Operationen auf ein gedachtes Bankkonto ein, von dem teurere dann abheben können. Zu jeder $i$-ten von insgesamt $n$ Einzeloperationen $o_{i}$ sind also nicht nur die tatsächlichen Kosten $c_{i}$ zur Durchführung entscheidend, sondern auch noch die amortisierten Kosten $a_{i}$, welche die Auswirkung auf das Bankguthaben $G$ beinhalten. Es gilt $a_{i} = c_{i} + G_{i} - G_{i-1}$. Wenn nun nach jeder durchgeführten Operation, dass Guthaben $>=0$ ist, wurde die gewählte obere Laufzeitschranke durchgängig eingehalten. Es muss also $\sum \limits_{i=1}^n (a_{i} - c_{i})  \geq 0$ für alle $n \in \mathrm{N}$ gelten. Damit das funktionieren kann, muss es natürlich einen Zusammenhang zwischen der Anzahl bereits durchgeführter günstiger Operationen und den tatsächlichen Kosten der teuren Operationen geben. 

\noindent Als Beispiel wird häufig ein Stack verwendet, welcher zusätzlich zu push und pop noch popAll anbietet. Bei popAll wird solange pop wiederholt, bis der Stack leer ist. Sei $n$ die Anzahl der bereits durchgeführten Operationen. Die teuerste Operation ist popAll mit $O(n)$, da hier bis zu $n$ mal pop durchgeführt werden muss. Push und pop können in konstanter Zeit durchgeführt werden. Ohne amortisierte Analyse würde man für $n$ Operationen $O(n^2)$ als obere Schranke angeben, da man jeweils den worst-case verwenden muss.
\noindent Für eine Analyse nach Bankkontomethode legt man nun fest, dass z.B die Kosten für push $c_(push) = x$ und pop $c_(pop) = y$  GE (Geldeinheit) kosten. Außerdem wird festgelegt, dass push y GE auf das Guthaben G einzahlt. Damit hat push konstante amortisierte Kosten  $a_(push)$ von $x + y$ GE. Im Gegenzug darf pop y GE vom Konto entnehmen, was amortisierte Kosten $a_(pop)$ von $y - y = 0$ GE ergibt. Wichtig ist die Beobachtung $(B1)$, dass bei einem Stack, der direkt nach der Erzeugung zunächst mal leer ist, genau so viele GE am Konto liegen, wie Elemente am Stack sind. Wenn pop ausgeführt werden kann sind also auch immer ausreichend GE am Konto. Da die Kosten für pop 0 GE sind, ist die teuerste der drei Operationen in diesem Beispiel nun push mit x GE. Eine Folge von n Operationen kostet also maximal $nx$, mit konstantem x. Damit kann man  $O(n)$ als asymptotische Laufzeitschranke verwenden. Da für den Wert des Bankkontos nach jeder Operation  $\geq 0$ gilt, hat der gedachte Stack nach jeder abgeschlossenen Operation maximal gleich viel Zeit benötigt, wie der reale Stack, so dass auch für diesen   $O(n)$ angenommen werden darf.  Abbildung \ref{fig:bankkontomethode} veranschaulicht das Beispiel nochmal.     
\begin{figure}[h]
	\centering
	\includegraphics[width=0.85\textwidth]{"bilder/bankkontomethode"}
	\caption{Bankkontomethode am Beispiel eines Stacks. }
	\label{fig:bankkontomethode}
\end{figure}  



\subsubsection{Potentialfunktionmethode}
Wenn nicht anders angegeben verwendet dieser Abschnitt die gleichen Bezeichner wie der Abschnitt Bankkontomethode. Die Potentialfunktionmethode ist der Bankkontomethode sehr ähnlich. Bei ihr wird jedoch nicht mit einem gedachten Bankkonto gearbeitet, sondern es wird der aktuelle Zustand der Datenstruktur bewertet. Die Bewertung der Datenstruktur übernimmt die Potentialfunktion $\Phi$. Man kann mit der Potentialfunktionmethode also beliebige Startzustände einer Datenstruktur verarbeiten, während man bei der Bankkontomethode einen Zustand benötigt für den $G_(DS) = 0$ gilt. Eine hohe Bewertung steht dabei für hohe potentielle Kosten. Das heißt Operationen die sich negativ auf den möglicherweise folgenden worst-case-Fall auswirken, verändern die Bewertung ins positive, und umgekehrt. Die amortisierten Kosten einer Operation werden mit $a_{i} = c_{i} + \Phi_{i} - \Phi_{i-1}$   angegeben. Man kann die amortisierten Gesamtkosten mit $ (\sum \limits_{i=1}^n c_{i}) + \Phi_{n} + \Phi_{0}$ zusammenfassen. Bis auf $ \Phi_{n}$ und $ \Phi_{0}$ heben sich die Elemente gegenseitig auf. Die amortisierten Kosten sollen wieder als obere Schranke für die tatsächlichen Kosten $\sum \limits_{i=1}^n c_{i}$ genutzt werden. 
Also muss $\forall m \in \{0..n\}: \sum \limits_{i=1}^m c_{i} \leq \sum \limits_{i=1}^m a_{i} $ gelten. Dies erreicht man indem nur Potentialfunktionen verwendet werden, bei der für eine leere Struktur $\Phi_{leer} = 0$ und  $\Phi_{i} \geq 0$ für alle $i$ mit $0 \leq i \leq n$ gelten. Das entscheidende ist nun zu einer Datenstruktur eine geeignete Potentialfunktion $\Phi$ zu finden. Das soll wieder am Stack mit drei Operationen gezeigt werden. 

\noindent Beim Stack bietet sich $\Phi_{i} = y$  $*$ $Anzahl$ $der$ $Elemente$ $im$ $Stack$ nach der Operation $i$ an. Dass die beiden Bedingungen  $\Phi_{i} \geq 0$ für alle $i$ mit $0 \leq i \leq n$ und $\Phi_{0} = 0$ erfüllt sind, ist damit schon mal direkt ersichtlich. Als amortisierte Kosten der Operationen ergeben sich $x + \Phi_{i} - \Phi_{i-1} = x + y$ für push, 
$y + \Phi_{i} - \Phi_{i-1} = y - y = 0 $ für pop. Für popAll ergibt sich (Anzahl Elemente am Stack) $*$ $0 = 0$. Am teuersten ist also push. Nun folgt eine Laufzeitabschätzung der amortisierten Kosten bei n Operationen im worst-case. $(\sum \limits_{i=1}^n c_{i}) + \Phi_{n} = nx + ny = n(x+y)$. Man darf also $O(n)$ als obere Schranke für die amortisierten Kosten angeben und diese sind eine obere Schranke für die tatsächlichen kosten.



\subsubsection{Amortisierte Kosten von splay }
Zu jedem Schlüssel des Baumes $i$ gehört ein Gewicht $iw(i)$, welches eine reelle Zahl $>1$ darstellt. Jeder Knoten $x$ ist Wurzel eines Teilbaumes $t_x$ und hat ein Gesamtgewicht $tw(x) = \sum \limits_{i \in t_x} iw(i)$. Der Wert von $tw(x)$ entspricht also der Summe aller Gewichte im jeweiligen Teilbaum. Der Rang $r(x)$ eines Knoten ist definiert durch $r(x) = log_2(tw(x))$. Sei $T$ der Gesamtbaum, dann ist die Potentialfunktion durch $\phi = \sum \limits_{n \in T } r(n)$ definiert. Damit gilt für eine leere Datenstruktur $\phi_(leer) = 0$. Außerdem gilt für jede andere Konstellation $\phi_(T) \geq 0$.
Intuitiv ist sofort ersichtlich, dass man niedriges Potential erreicht, wenn der Baum gut balanciert ist und schwere Knoten weit oben sind. 
\noindent Im folgendem Lemma aus \cite{sl2} von Daniel Dominic Sleator und  Robert Endre Tarjan  wird diese Rangeigenschaft verwendet: Wenn Knoten $p$ Vater der Knoten $s$ und $t$ ist, mit $r(s) = r(t)$,  dann gilt $r(p) > r(s)$. Das folgt aus $tw(s) \geq 2^{r(s)}$ und $tw(t) \geq 2^{r(t)}$, da $tw(p) \geq tw(s) + tw(t) \geq 2^{r(s) + 1}$.
\\\\
\noindent\textbf{Lemma \cite{sl2} :} \textit{Eine splay Operation mit Knoten x in einem Baum mit Wurzel v hat maximal  amortisierte Kosten $a$ von $3 (r (v) -r (x)) + 1$. Dies wird auch als Zugriffslemma bezeichnet.}
\\
\noindent\textbf{Beweis} Mit $x$ wird der Knoten bezeichnet, der an splay übergeben wurde. $y$ ist der Vater von $x$ und $z$ ist der Vater von $y$. $r()$, $tw()$ und $\phi$ liefern die Funktionswerte für die Struktur vor der Rotation, $r' ()$, $tw'()$ und $\phi'$ die Werte für danach. Die Kosten für das Ausführen von Rotationen sind konstant und werden im folgenden mit $1$ angegeben.
Zunächst wird für jeden Rotationstyp gezeigt, dass die Kosten niedriger als  $3 (r '(x) -r (x)) + 1$ sind.
\\
\noindent\textbf {zig bzw. zag Rotation}:
Da diese Rotation nur direkt unter der Wurzel ausgeführt wird, ist $z$ nicht definiert.
Es gilt also $\phi' - \phi = r'(x) + r'(y) - r(x) - r(y)$.  
Nach Abschluss der Rotation ist $x$ die Wurzel, zuvor war es $y$, daraus folgt $r(y) = r'(x)$ und  $\phi' - \phi = r'(y) - r(x)$. Durch $r'(x) \geq r'(y)$ ergibt sich $\phi' - \phi \leq r'(x) - r(x) \leq 3 (r '(x) -r (x))$. Mit den Kosten für das durchführen der Rotation erhält man  $1 + \phi' - \phi \leq 1 + r'(x) - r(x) \leq 3 (r '(x) -r (x) + 1)$.

\noindent\textbf {zigZig bzw. zagZag Rotation}:
Hier ist auch $z$ definiert und es gilt  $\phi' - \phi = r'(x) + r'(y)+ r'(z) - r(x) - r(y) - r(z)$. Analog zum zig Fall gilt  $r(z) = r'(x)$, also   $\phi' - \phi =r'(y)+ r'(z) - r(x) - r(y)$. Mit $r'(x) \geq r'(y), r'(x) \geq r'(z)$ und $r(x) \leq r(y) $, gilt $\phi' - \phi \leq 2(r'(x) - r(x))$. Hat sich der Rang von x erhöht, kann die Differenz von $3(r'(x) - r(x))$ zu $2(r'(x) - r(x))$ genutzt werden um die Rotation auszuführen. 
\noindent Gilt $r'(x) = r(x)$, muss auch $r'(x) = r(y) = r(z) = r(x)$ gelten, denn $r(x) \leq r(z)$, $r(x) \leq r(y)$,  $r'(x) \geq r(z)$ und  $r'(x) \geq r(y)$ . Dass auch $r'(z) < r(x)$ gilt erkennt man gut am Zwischenschritt der zigZig Rotation, siehe Abbildung \ref{fig:zigZigRotation}. Nach diesem Zwischenschritt ist x immer noch Wurzel des gleichen Baumes, wie zu Beginn. z ist bereits die Wurzel des Baumes, die es auch nach Ende der Rotation noch ist. Außerdem ist der Rang von y nach dem Zwischenschritt gleich $r'(x)$. Daraus folgt mit der vor dem Lemma beschriebenen Rangeigenschaft $r'(z) < r(x) = r(z) $. Aus $r'(x) = r(x)$,  $r'(y) \leq r(y)$ und 
$r'(z) < r(z) $ ergibt sich aber $\phi' < \phi$, und mit dieser Differenz kann die Rotation bezahlt werden. 
\noindent Zusätzliche Kosten für das Ausführen der Rotation entstehen also nicht.

\noindent\textbf {zigZag bzw. zagZig Rotation}:
\noindent Aus den gleichen Gründen wie bei zigZig gilt  $\phi' - \phi =r'(y)+ r'(z) - r(x) - r(y) \leq 2(r'(x) - r(x))$. In Abbildung \ref{fig:zigZagRotation} sieht man das im Teilbaum mit Wurzel y keine neuen Knoten hinzukommen, also gilt $r'(y) \leq r(y)$. Ebenfalls aus der Abbildung \ref{fig:zigZagRotation} kann man $r'(z) \leq r'(x)$ entnehmen, woraus  $\phi' - \phi \leq r'(x) - r(x)$ folgt. Für $r'(x) \geq r(x)$ können die Kosten für die Ausführung nun wieder aus der Differenz zu $3(r'(x) - r(x))$ bezahlt werden.
\noindent Für $r'(x) = r(x) = r(z) = r(y)$  gilt aufgrund der Rangeigenschaft von oben, dass mindestens einer der Knoten z und y im Rang gefallen sein muss. So dass man auch hier einen Kredit zum durchführen der Arbeit über hat. Auch der zigZag Fall benötigt also die zusätzliche KE nicht.

\noindent Für aufsummierte zigZig und zigZag Rotationen gilt also $a \leq 3(r(v) - r(x) )$. Bei der letzten Rotation, die $x$ zur Wurzel macht kommt dann $1$ hinzu, somit ist man bei  $a \leq 3(r(v) - r(x)) + 1 $. Mit der bekannten Regel $log_x y - log_x z = log_x \frac{y}{z}$, ergibt das $a \leq 3(log_2 \frac{tw(v)}{tw(x)})$\\

\noindent Das Ergebnis dieser Analyse ist auch als Zugriffslemma bekannt:

\noindent\textbf{Zugriffslemma \cite{sl2}:}\\
\textit{Die amortisierte Laufzeit der Splayoperation mit Wurzel t und Knoten x ist maximal  $3( r(t) - r(x)) + 1 = O( log(( t)/s(x))$ } 



\subsubsection{Amortisierte Kosten der anderen Operationen}
Für die Analyse der anderen Operationen wird zu Beginn von einem Wald aus leeren Bäumen ausgegangen. Zu keinem Zeitpunkt ist ein Schlüssel in mehr als einem Baum enthalten. $U$ steht für die Menge aller möglichen Schlüssel. $T$ für die Menge aller im Wald enthaltenen Bäume. $V$ ist die Menge der Schlüssel $i$ mit $i \in U$ und  	$\forall (t \in T)(i \notin t) $. Als Potentialfunktion wird die Summe der Potentiale aller Bäume und der logarithmiertem Gewichte von aktuell nicht enthaltenen Gewichten verwendet. $\phi = \sum\limits_{t \in T} \phi_{t} + \sum\limits_{i \in V} log(i) $. Zu Beginn hat man also das kleinstmögliche Potential $p_0 = \sum\limits_{i \in U} log(i) > 0$. Für einen Schlüssel $i$ aus dem Baum $t$ sei $i+$ der nächst größere Schlüssel in t, $i-$ der nächst kleinere. Ist $i-$ bzw.  $i+$ nicht definiert gilt $w(i-) = \infty$ bzw. $w(i+) = \infty$. Sei außerdem $tw(i)$ die Gesamtgewichtsfunktion vor der Ausführung und  $tw'(i)$ die Gesamtgewichtsfunktion nach der Ausführung. Das folgende Lemma aus \cite{sl2} Daniel Dominic Sleator und  Robert Endre Tarjan gibt nun Aufschluss über die genauen Kosten. 

\noindent\textbf{Lemma \cite{sl2} : }\textit{Es sei $W$ das Gesamtgewicht der an einer Operation beteiligten Bäume, dann gelten die amortisierten oberen Laufzeitschranken aus Abbildung \ref{fig:kostenOps}}. \\

\begin{figure}[H] 
	\centering
	\includegraphics[width=0.85\textwidth]{"bilder/kostenOps"}
	\caption{Amortisierte Kosten beim Splaybaum \cite{sl2}. }
	\label{fig:kostenOps}
\end{figure}  
\noindent\textbf{Beweis:} 
Die Angeben für suchen und aufteilen ergeben sich direkt aus dem Zugriffslemma. Für vereinigen ergibt sich als Potentialänderung $log \frac{tw(t1) + tw(t2)}{tw(t1)}  \leq  3$ $log (\frac{W}{tw(t1)})$. Außerdem gilt $tw(t1) \geq w(i)$.
Bei einfügen hat man zunächst die Kosten von suchen. Da der eingefügte Schlüssel $i$ zuvor mit $log(i)$ in der Gesamtpotentialfunktion enthalten war ergibt sich zusätzlich $log \frac{w(i) + tw(t)}{w(i)} = log(\frac{W}{w(i)})$.
Die Schranke von löschen ergibt sich aus denen von suchen und verbinden.


\section {Weiterführende Eigenschaften des Splaybaum}
Es kommt bei den folgenden Unterpunkten vor, dass Gewichte an die Knoten fest vergeben werden. Dies kann ohne Probleme gemacht werden da das Verhalten des Splaybaum durch die ausgeführten Operationen bestimmt wird. Da die Gewichte in den Operationen nicht verwendet wird, ist das Verhalten von den gewählten Werten unabhängig. So dass man sie frei zur Analyse verwenden kann. Die beiden Sätz in diesem Kapitel stammen aus \cite{sl2} von Daniel Dominic Sleator und  Robert Endre Tarjan. 


\subsection{Balance Satz \cite{sl2}:} 
Beim diesem Satz geht es um die amortisierte Gesamtzugriffszeit auf einen Baum mit $n$ Knoten und Wurzel $r$. Die Gesamtgewichtsfunktion eines Knoten mit Schlüssel i hat ein Minimum $w(i)$ und ein Maximum von $\sum \limits_{i = 1}^n w(i) $. Die Wert der Potentialfunktion kann sich deshalb maximal um $\sum \limits_{i = 1}^n  log \frac{tw(r)}{w(i)} $ absenken. \\
\noindent\textbf{Balance Satz \cite{sl2}: }\textit{Es sei ein Splaybaum mit $n$ Knoten geben auf den $m$ mal zugegriffen wird. Dann gilt für die Gesamtzugriffszeit $O((m + n)log$ $n + m)$} \\

\noindent\textbf{Beweis:} 
Es wird $\frac{1}{n}$ als Gewicht für jeden vorkommenden Schlüssel gewählt. Mit dem Zugriffslemma kann man die Kosten einer Einzeloperation mit $\leq$ $3$ $log (n) + 1$, als Sequenz also $O ($m $log$ $(n) +m)$, angeben. Damit erreicht man, dass man nun nicht mehr jeden Einzelschritt nachverfolgen muss, um den aktuellen Rang des Zugriffsknotens zu wissen. Addiert man dazu die maximale negative Potentialänderung $O$ $(n$ $log$ $\frac{1}{\frac{1}{n}})$ $=$ $O$ $(n$ $log$ $n)$, erhält man die Behauptung. \\

\noindent Ein Suchbaum der immer bestmöglich in Balance ist, erreicht $O(m$ $log(n))$. Da $n$ konstant ist, ist der Splaybaum amortisiert betrachtet, asymptotisch genau so gut. 
 
  
  
 \subsection{Statische Optimalität}
Bei statischen binären Suchbäumen kommt es ohne explizite Updateoperation zu keiner Strukturänderung. 
Wenn die Zugriffswahrscheinlichkeiten auf die Elemente eines solchen  Suchbaumes bekannt sind, dann lässt sich ein optimaler statischer Suchbaum $T_o$ bezüglich der Kosten für suchen konstruieren. Wenn also  die Elemente $E_1$, $E_2$...,$E_n$ vorhanden sind und diese Zugriffswahrscheinlichkeiten $p_1$, $p_2$...,$p_n$ zugeordnet sind, dann lassen sich die Zugriffskosten für jeden möglichen Baum mit $C_t = \sum\limits_{i=1}^n p_i (Ebene(E_i) + 1)$ berechnen. Beim statisch optimalen Suchbaum ist dieser Wert dann minimal. Als Beispiel dienen Bäume mit den Schlüsseln 1, 2 und 3 und Wahrscheinlichkeiten von $p(1) = 0.1$, $p(2) = 0.7$ und $p(3) = 0.2$, so dass $p(1) + p(2) + p(3) = 1$ gilt. Es gibt fünf mögliche Suchbäume mit diesen Elementen, siehe Abbildung \ref{fig:5trees}.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.85\textwidth]{"bilder/5trees"}
	\caption{Fünf mögliche Suchbäume mit den Schlüsseln 1,2 und 3 }
	\label{fig:5trees}
\end{figure}  
Der statisch optimale Baum für diese Werte ist der ganz linke mit $C_t = 1.1$.
Der folgende Satz sagt nun aus, dass die Laufzeit eines Splaybaumes asymptotisch mindestens genau so gut ist, als die eines jeden statischen Suchbaumes, also die des statisch optimalen Suchbaumes eingeschlossen. \\
\\
\noindent\textbf{Satz zur statischen Optimalität \cite{sl2}:}\\
\textit{Es sei $q(i)$ die Anzahl der Zugriffe auf den Schlüssel $i$. Gilt für jeden Schlüssel $i$, $q(i) \geq 1  $. Dann gilt für die Kosten k des Gesamtzugriffes $k$ $=$ $O(m + \sum\limits_{i = 1}^n  q(i)log(\frac{m}{q(i)})$}). \\

\noindent\textbf{Beweis:}
Es wird ein Gewicht $w(i)$ von $q(i)/m$ gewählt. Der Maximalwert für $tw(i)$ ist dann $1$. Es wird das Zugriffslemma verwendet um die Kosten für einen Zugriff auf $i$ mit $O(log(\frac{1}{tw(i)}))$ anzugeben.
Da $q(i) \geq 1$, gilt $mq(i) \geq m  \geq tw(i)$. Damit gilt $ \frac{m}{q(i)} \geq \frac{1}{tw(i)} $ und es kann  $O(m(log(\frac{m}{q(i)}))$ = $O(m)$ als Schranke für alle $m$ Zugriffe verwendet werden. Das Potential kann sich maximal um $\sum\limits_{i = 1}^n log(\frac{m}{q(i)})$  vermindern. Aufaddieren ergibt dann die Behauptung.  \\

\noindent Die Hauptidee des Beweisen ist es, die Gewichte $iw(x)$ der Elemente des Splaybaumes abhängig von der Zugriffswahrscheinlichkeit zu machen. Damit erreicht die Baumstruktur genau dann niedriges Potential, wenn die oft angefragten Elemente weit oben sind und der Baum gut balanciert ist. Dass die bewiesene Schranke ausreicht um die asymptotische Schranke des optimalen statischen Suchbaumes einzuhalten, kann in der Arbeit \cite{ab} von Norman Abramson nachvollzogen werden.

 
 
 \subsection{Dynamische Optimalität}
Dynamische binäre Suchbäume können ihre Struktur auch infolge von Zugriffen auf Elemente des Suchbaumes ändern. Der Splaybaum ist also ein dynamischer Suchbaum.

\noindent Sei $A$ ein binärer Suchbaumalgorithmus mit folgenden Eigenschaften:\\
1. $A$ startet bei Zugriff auf eines seiner Elemente $x$ mit einer Suche ab der Wurzel.\\
2. $A$ muss den Pfad von der Wurzel zum Element $x$ komplett ablaufen.\\
3. $A$ kann den Zugriff auf einen Knoten über einen Zeiger in $O(1)$ Zeit ausführen.\\
4. Rotationen kann $A$ in $O(1)$ Zeit ausführen.\\
5. Zwischen denn Zugriffen auf einzelne Knoten kann $A$ beliebige Rotationen durchführen.\\



\noindent Als dynamisch optimal gilt ein binärer Suchbaumalgorithmus, wenn er für jede Suchfolge asymptotisch mindestens genau so gut ist, wie jeder Suchalgorithmus A. 

\noindent In \cite{sl2} von Daniel Dominic Sleator und  Robert Endre Tarjan wird vermutet, dass Splaybäume dynamisch optimal sind.




\bibliographystyle{plain}
\bibliography{litVer}


\end{document}